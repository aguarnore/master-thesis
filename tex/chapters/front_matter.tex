% Abstract
\abstract
{
Reinforcement learning theory ensures convergence in environments with finite state and action spaces. However, real-life environments often involve large and continuous state and action spaces, which require approximate methods able to learn states not yet visited. Unfortunately the theoretical guarantees for such methods are not as robust.

In this work we review the theory of the fundamental reinforcement learning methods. Then, we show how in practice, in the finite case, various methods and parameters can affect convergence rate. In the continuous case, we first present results obtained by discretizing the state space, then subsequently show how better results can be achieved with approximate methods, while also considering once again the role of the parameters while learning.
}

% Dedication
% \dedication{}

% Acknowledgements
% \begin{acknowledgements}\end{acknowledgements}

% Table of contents
\tableofcontents
