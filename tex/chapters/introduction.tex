\chapter{Introduction}
Reinforcement learning (RL), along with supervised and unsupervised learning, is one of the three main machine learning paradigms. In RL an agent situated in an environment has to learn to behave optimally according to some notion of cumulative numerical reward. It differs from supervised learning, since learning is not guided by a set of examples labeled by a knowledgeable external supervisor, and it also differs from unsupervised learning since the agent does not seek hidden patterns in unlabeled data.

Part of RL can be seen as directly inspired from behaviorism, which attempts to understand human and animal behavior with respect to their response to stimuli that the environment provides. Likewise, RL concerns itself with how agents learn by iteratively interacting with an environment. In particular: the agent takes actions environment, then, in response the said action, the environment sends back to the agent a new representation of the state after the action, and a numerical reward associated with the action the agent took. Furthermore, one of the distinguishing characteristics of RL is the dilemma between exploration and exploitation which, in a sense, emulates learn by trial and error. In order to obtain a high reward, the agent has to choose actions which it has found to be effective in the past. However, in order to find such actions, it also has to try actions that it has not attempted previously.

More formally, the objective of the agent is that of finding an policy --- a mapping from states to actions --- which maximizes the expected cumulative reward, called value. In the case of finite state and action spaces, RL theory clearly establishes the conditions needed for each method to converge, while in the case of large or continuous state spaces, where approximate methods that update the value to states not yet visited are needed, only weaker convergence guarantees are given. In fact, when representing the value as linear combination of weights and features, the two most prominent value-based methods, SARSA and Q-learning, respectively converge near to the optimal solution and can exhibit instability. Alternatively, using a nonlinear representation, such as the more powerful neural networks, no convergence guarantees have been proven. In recent years interest has shifted towards a second class of methods, which instead directly optimize the policy. In general such policy-based methods benefit from better convergence properties, as well as the possibility to be employed with environments with continuous action spaces.

The aim of this work is to review the main RL methods, and compare the aforementioned theoretical results with empirical ones, for various classic environments. In Chapter~\ref{ch:optimal_control} we introduce the theory of optimal control, upon which the theory of reinforcement learning is built. Then, by removing the assumption of perfect knowledge of the environment, we start exploring reinforcement learning theory. Chapters~\ref{ch:value_based_rl} and~\ref{ch:policy_based_rl} respectively illustrate the most important model-free value- and policy-based reinforcement learning methods. In Chapter~\ref{ch:empirical_results} we report and discuss some empirical results obtained by running some experiments on the methods presented in the previous chapters. In particular, in the case of finite state spaces, we show how the agent's farsightedness and level of exploration over time, but also the environment and the method itself can affect learning, both in terms of convergence rate and variance. In the case of continuous state spaces we first present results obtained by discretizing the state space, and then compare them with agents trained on featurized states, with different parameters. Ultimately, we also consider the more complex case in which both the state and the action space are continuous. Chapter~\ref{ch:conclusions} concludes the thesis by providing some remarks regarding possible future developments and other relevant aspects that were not discussed in this work. The code with which the experiments have been conducted has been published in a GitHub repository\footnote{\url{https://github.com/andreaguarnore/reinforcement-learning}}.
