\chapter{Conclusions} \label{ch:conclusions}
In the first part of this work we introduced dynamic programming methods to solve optimal control problems. Then, by removing the assumption of perfect knowledge of the environment we presented more efficient methods, divided in value- and policy-based methods, also focusing on the more challenging cases of continuous state and action spaces.

The goal of the thesis was that of attempting to understand, empirically, which methods and parameters are best for a given task by testing a variety of them on different environments. As of now, no clear strategy for choosing them has been found. As such, the optimal methods and parameters can only be attained by manually testing many candidate combinations. Nevertheless, we saw that in the case of~${\varepsilon}$, although theoretically the class of scheduling strategies is wide, only a handful of them ensure convergence in practice. The choice of the scheduling strategy for~${\varepsilon}$ ultimately depends on both the specific problem at hand, and the number of training episodes. Additionally, we looked at how different methods, but also their parameters, such as the values of~${\robbinsmonro}$,~${n}$, and~${\lambda}$, can be interpreted as balancing terms between bias and variance. Ultimately we observed how, in the case of continuous state spaces, the choice of which features to use, and also their parameters, is once again not straightforward. Whenever possible, hand crafted problem-specific features outperform more generic ones, which instead require fine tuning.

% stuff not mentioned in the thesis

Lastly, we report some remarks with respect to future work in the field. First, reward signals strongly influences what and how the agent learns, yet currently no obvious guideline with respect to their design has been presented. Oftentimes, especially when the task is hard to translate to a reward signal, the agent can find a way to bypass the problem by acting in unintended ways, or even discovering new techniques~\cite{baker2019}. To address this shortcoming, several alternative approaches that include advice given by an external observer have been proposed~\cite{clouse1992teaching, maclin1994}. Second, although the methods presented here yield satisfactory on simpler environments with continuous state and action spaces, current research shows that we still lack approximate methods which are both online and incremental and that can be employed on very complex environments. In fact, deep learning methods applied to classic RL methods require workarounds in order to learn properly, such as batch training on large data sets~\cite{mnih2013dqn}, experience replay~\cite{lin1992}, and self play in the case of games~\cite{silver2016go}. The third and especially important aspect is that of formalizing curiosity in learning agents. Current reinforcement learning methods solve tasks known and fixed a priori, yet having agents be able to choose what to learn, possibly by having them include a signal representing how novel a state or an action is, has been shown to be beneficial in preliminary studies~\cite{schmidhuber1991a, schmidhuber1991b}. Of course, it is also relevant to remark that giving agents freedom to explore, while helpful for training, could also prove to be unsafe in industrial applications, or even unethical, as agents get closer to  artificial general intelligence.
